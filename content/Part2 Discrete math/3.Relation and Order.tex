\chapterimage{orange2.jpg}
\chapterspaceabove{6.75cm} 
\chapterspacebelow{7.25cm} 
\chapter{Relation and Order}

\section{Basic Relation and class}
In this chapter, we will discuss further topics on set theory, or more specifically, 
relations. With set as tool, we can categorize things and try to build connections 
between them, like defining a function from a preimage to an image. Relation is a superset
, i.e., generalization of function, which is crucial to topics that we will discuss
later in this chapter.

In real life, relation is referred to as some connections between one person or 
one group of people to the other. 
\begin{enumerate}
    \item Imagine a list of students and their grades in a class. Each student 
    (let's say, by their student ID) is linked to a specific grade. 
    This "student-to-grade" pairing is an example of a functional relation, 
    because every student has one and only one grade assigned.
    \item Consider the relationship "has the same birthday as" among people. If person A has the same birthday as person B, and person B has the same birthday as person C, then person A also has the same birthday as person C. This relationship is an equivalence relation because it's reflexive (everyone has the same birthday as themselves), symmetric (if A shares a birthday with B, then B shares a birthday with A), and transitive (if A shares a birthday with B, and B with C, then A shares a birthday with C).
    \item Think about the books on a shelf organized by height. Each book can be considered as "shorter than or equal to" the book next to it if you move from left to right. This arrangement demonstrates an order relation because it's reflexive (each book is the same height as itself), antisymmetric (if one book is both taller and shorter than another, they must be the same book), and transitive (if one book is shorter than a second, and the second is shorter than a third, then the first book is shorter than the third).
\end{enumerate} 
The notion is still quite similar in the context of
mathematics. From many examples we can see this. Like all the mathematical operations
we have defined, the mapping in a function, congruence...
To sum up, relation is am abstract topic, yet not hard to understand, wince it can be
related to the material world easily. 
\subsection{Class}
Additionally, we introduce a new concept related to set to explain what is relation.
The set theory we discussed in the first part of this book is called \textbf{Naive Set Theory}, which Is
actually not the modern set theory. In many aspects, it is not reliable and causes a lot of issues.
A very famous example is Russell's Paradox initiated by the British Philosopher and Mathematician.

Russell's Paradox illustrates a significant problem in the naive set theory, 
which assumed sets could include themselves. The paradox is encapsulated in whether the 
"set of all sets that do not contain themselves" contains itself. 
If it contains itself, it contradicts its defining property. Conversely, if it does not contain 
itself, then by definition, it must contain itself. This dilemma indicated the limitations 
of the naive set theory approach, leading to contradictions. To address these issues, the 
concept of classes was introduced in \href{https://www.wikiwand.com/en/Von_Neumann%E2%80%93Bernays%E2%80%93G%C3%B6del_set_theory}{\textbf{NBG Set Theory} (von Neumann-Bernays-Gödel set theory)}. 

Classes allow for the conceptualization of collections too large or abstract to be considered as sets, thereby circumventing the paradoxes 
associated with a more naive interpretation of set theory.
\begin{remark}
    An interesting fact is that the von Neumann here is exactly \href{https://www.wikiwand.com/en/John_von_Neumann}{John von Neumann},
    who initiated von Neumann Architecture for computer. It seems that a great Computer Scientist is always also a great Mathematician.
\end{remark}
\begin{definition}[Class]
    A \emph{class} is a collection of objects that are grouped together based on a shared property. 
    Classes differ from sets in that they can represent collections of any size, including 
    those too large to be considered as sets, such as the "class of all sets". 
    This concept is essential for avoiding paradoxes in set theory, 
    allowing the discussion of large and abstract collections that cannot 
    otherwise be accommodated within the framework of sets.
\end{definition}

We also introduce some of the axioms that is really important for the content here, they are
easy to understand, but are important prerequisite for our further discussion.
\begin{axiom}[Axiom of Extensionality]
    For two classes $x, y$, we have $x=y$ if and only if $\forall z, z\in x \equiv z\in y$.
    In brief, two classes are equal if and only if every member of each is also
    a member of the other.
\end{axiom}

The other Axiom is on creating new classes.
\begin{axiom}[Axiom Scheme of Classification]
    For each open sentence \( P(x) \) there exists a class which consists precisely of those sets which satisfy the condition \( P(x) \).
    
    The class whose existence is postulated by the Axiom is denoted by \( \{x : P(x)\} \); thus 
    \( \{x : P(x)\} \) is a term of the theory NBG and the assertion \( u \in \{x : P(x)\} \) is true if 
    and only if \( u \) is a set and \( P(u) \) is true.
\end{axiom}
\subsubsection*{Properties and Operations of Class}
Since the definition of Class and set are related, there are a lot of overlap in their properties.
The union and intersection of two classes are defined in exactly the same way as the union and intersection of two sets in naïve set theory: if \( A \) and \( B \) are classes, then
\begin{align*}
    A \cup B &= \{x : (x \in A) \vee (x \in B)\}, \\
    A \cap B &= \{x : (x \in A) \wedge (x \in B)\}.
\end{align*}
So a set \( x \) is a member of \( A \cup B \) if and only if it is a member of either \( A \) or \( B \) (or both); \( x \) is a member of \( A \cap B \) if and only if it is a member of both \( A \) and \( B \).

The usual properties of these unions and intersections are established as in naïve set theory. Namely, we have the properties known as idempotence,
\begin{align*}
    \forall X(X \cup X = X), \quad \forall X(X \cap X = X),
\end{align*}
associativity,
\begin{align*}
    \forall X \forall Y \forall Z (X \cup (Y \cup Z) = (X \cup Y) \cup Z), \\
    \forall X \forall Y \forall Z (X \cap (Y \cap Z) = (X \cap Y) \cap Z),
\end{align*}
commutativity,
\begin{align*}
    \forall X \forall Y (X \cup Y = Y \cup X), \quad \forall X \forall Y (X \cap Y = Y \cap X),
\end{align*}
and distributivity,
\begin{align*}
    \forall X \forall Y \forall Z (X \cup (Y \cap Z) = (X \cup Y) \cap (X \cup Z)), \\
    \forall X \forall Y \forall Z (X \cap (Y \cup Z) = (X \cap Y) \cup (X \cap Z)).
\end{align*}

\begin{definition}[Complement of Class]
We write \( x \notin y \) as an abbreviation for \( \neg(x \in y) \). Then for each class \( A \) we define the complement of \( A \) to be the class
\begin{align*}
    \sim A = \{x : x \notin A\}.
\end{align*}
\end{definition}

\begin{definition}[Difference of Classes]
For two classes $A$ and $B$, the difference $A~B$ is defined as:
$$
A\sim B=\{x:(x\in A)\land(x\not\in B)\}=A\cap(\thicksim B).
$$
\end{definition}
Also, note that double negation and De Morgan's Law also work for class.

As what we have discussed in set theory that there exist some empty sets, we also have null class and
universe class.
\begin{definition}[Null Class and Universe Class]
    Empty class $\emptyset$ and universe class $V$ are defined by
    $$\emptyset=\{x:x\neq x\}\ \ \ V=\{x:x=x\}.$$
\end{definition}
Classes also have their exclusive operations, including intersection and union.
\begin{definition}[Intersection and Union of Class(of one single class)]
    Let $A$ be a class; the union and intersection of the class $A$ are
    the classes
    $$
    \begin{array}{rcl}\bigcup A&=&\{x:(\exists y)((y\in A)\land(x\in y))\},\\
        \bigcap A&=&\{x:(\forall y)((y\in A)\to (x\in y))\}\end{array}
    $$
    Where $x$ is a set and $y$ is a class.
\end{definition}
Thus a class $C$ belongs to $\bigcup A$ if and only if $C$ is a set and $C$ belongs to at least one of the 
members of $A;C$ belongs to $\bigcap A$ if and only if $C$ is a set and $C$ belongs to every member of $A.$

The definition is quite different from intersection and union of sets, as class is a generalization
of set from higher level abstraction. Also, the intersection and union for sets are binary operations,
while unary for one class. 

Below is a brief comparison.

\textbf{Set Union and Intersection:}
\begin{enumerate}
\item For two sets $A$ and $B$, their union $A \cup B$ is the set of all elements that belong to either $A$ or $B$. Formally: $A \cup B = \{x : x \in A \lor x \in B\}$.
\item For two sets $A$ and $B$, their intersection $A \cap B$ is the set of all elements that belong to both $A$ and $B$. Formally: $A \cap B = \{x : x \in A \land x \in B\}$.
\end{enumerate}

\textbf{Class Union and Intersection:}
\begin{enumerate}
\item For a class $A$, the class union $\bigcup A$ is the set of all elements $x$ such that there exists a class $y$, where $y$ is a member of $A$, and $x$ is an element of $y$. Formally: $\bigcup A = \{x : (\exists y)((y \in A) \land (x \in y))\}$.
\item For a class $A$, the class intersection $\bigcap A$ is the set of all elements $x$ such that for every class $y$, if $y$ is a member of $A$, then $x$ is an element of $y$. Formally: $\bigcap A = \{x : (\forall y)((y \in A) \to (x \in y))\}$.
\end{enumerate}

\textbf{Comparison:}
\begin{enumerate}
\item Set union and intersection operate on two sets, while class union and intersection operate on all member sets of a class.
\item Set union includes elements that are in either $A$ or $B$, while class union includes elements that are in at least one member of $A$.
\item Set intersection includes elements that are in both $A$ and $B$, while class intersection includes elements that are in all members of $A$.
\item The result of set union and intersection is always a set, while the result of class union and intersection is not necessarily a set
, but more likely a class that contains sets.
\end{enumerate}

By comparing these concepts, we can see that class union and intersection are generalizations of set 
operations at a higher level of abstraction. They allow us to perform operations on the member sets of a 
class to obtain new sets, which is particularly useful when studying advanced topics in mathematical 
foundations and set theory.

There are several lemmas related to intersection and union of class.

\begin{lemma}
    $\bigcap \emptyset= V$ and $\bigcup \emptyset= \emptyset.$ 
    \end{lemma}
    \begin{proof}
        Let \( C \) be a class. Then we have
        \[
        C \in \bigcap \emptyset \iff C \text{ is a set and } C \text{ belongs to every member of } \emptyset
        \]
        Since $\emptyset$ does not literally have any member.
        \[
        \iff C \text{ is a set}
        \]
        Any set is a member of universe class
        \[
        \iff C \in V.
        \]
        Thus
        \[
        \bigcap \emptyset = V
        \]
        by the Axiom of Extensionality.

        Again let \( C \) be a class. Then
        \[
        C \in \bigcup \emptyset \iff C \text{ is a set and there is a member } x \text{ of } \emptyset \text{ such that } C \in x
        \]
        \[
        \iff C \in \emptyset
        \]
        (since \( \emptyset \) has no members).

        So
        \[
        \bigcup \emptyset = \emptyset
        \]
        by the Axiom of Extensionality.
    \end{proof}
    The inclusive relation of class is very similar to set.
    \begin{definition}[Inclusion of Classes]
        If \( A \) and \( B \) are classes such that every member of \( A \) is also a member of \( B \), 
        i.e., such that we have
        \[
        \forall x ((x \in A) \to (x \in B)),
        \]
        we say that \( A \) is included in \( B \), \( B \) includes \( A \) or \( A \) is a subclass of 
        \( B \), and we write \( A \subseteq B \) or \( B \supseteq A \). (If \( A \) is a set and 
        \( A \subseteq B \) we say that \( A \) is a subset of \( B \).) If \( A \subseteq B \) and there 
        is at least one set \( b \) such that \( b \in B \) but \( b \not\in A \), we say that \( A \) is 
        properly included in \( B \), \( B \) properly includes \( A \) or \( A \) is a proper subclass of 
        \( B \), and we write \( A \subset B \) or \( B \supset A \).
        \end{definition}

        We can also extend power set to power class.
        For every class \( A \) we define the power class \( P(A) \) of \( A \) to be the class of all subsets of \( A \), i.e., \( P(A) = \{x : x \subseteq A\} \).
        This brings us to another axiom in NBG set theory.
        \begin{axiom}[Power Set Axiom]
        For every set \( x \) there exists a set \( y \) such that \( u \in y \) if and only if \( u \subseteq x \).
        \end{axiom}
        
        The Power Set Axiom thus asserts that every subclass \( u \) of a set \( x \) is actually a set (since it is an element of the set \( y \) whose existence is asserted by the axiom) and furthermore that the power class \( P(x) \) of a set \( x \) is also a set (and so is usually referred to as the power set of \( x \)).
        
        The rest of the axioms are as follows.
        \begin{axiom}[Pairing Axiom]
        For all sets \( x \) and \( y \) the class \( \{z : (z = x) \vee (z = y)\} \) is a set.
        \end{axiom}
        
        The set \( \{z : (z = x) \vee (z = y)\} \) is denoted by \( \{x, y\} \) and such a set is called an 
        unordered pair. If \( x = y \) the unordered pair \( \{x, y\} \) is denoted by \( \{x\} \) and is 
        called singleton \( x \).
        \begin{remark}
            In set theory, an \emph{unordered pair} refers to a collection of two elements in which the sequence of the elements does not matter. This is represented as $\{a, b\}$, indicating a set that contains exactly two distinct elements $a$ and $b$. The fundamental property of unordered pairs is that $\{a, b\} = \{b, a\}$, asserting that the order of elements is immaterial.

The concept of an unordered pair is not limited to sets; it extends to classes in certain set theories that distinguish between sets and proper classes. While sets are collections of elements that themselves can be elements of other sets, proper classes are collections too large to be sets and hence cannot be elements of other collections. Nonetheless, the notion of grouping two objects into an unordered pair applies analogously, symbolizing the collection of those objects without regard to order.
        \end{remark}
        \begin{axiom}[Union Axiom]
        For every set \( x \) the class \( \bigcup x \) is a set.
        \end{axiom}

        We can also extend Cartesian Product to class.
        Let $a$ and $b$ be sets. Then the set $\{\{a\},\{a,b\}\}$ is denoted by $(a,b)$ and is called the ordered pair with first coordinate $a$ and second coordinate $b$. Lét $A$ and $B$ be classes; then the Cartesian product of $A$ and $B$ is the $c$lass

        $$
        A\times B=\{t:(\exists x)(\exists y)((x\in A)\land(y\in B)\land(t=(x,y)))\},
        $$

        i.e. $A\times B$ is the class of all ordered pairs with first coordinate in $A$ and second coordinate in $B.$

        If $P(x,y)$ is an open sentence involving the free variables $x$ and $y$ we shall allow ourselves to write

        $$
        \{(x,y):P(x,y)\}
        $$

        as an abbreviation for

        $$
        \{t:(\exists x)(\exists y)((t=(x,y))\land P(x,y))\}.
        $$

        So we can abbreviate the definition of $A\times B$ to

        $$
        A\times B=\{(x,y):(x\in A)\wedge(y\in B)\}.
        $$


\subsection{Binary Relations, Composition and Inverse}
In mathematics, the most fundamental and ubiquitous type of relation is the \emph{binary relation}. This term refers to a relationship between two objects or sets. We understand that a `set' may encompass any concept, not solely in the mathematical sense but also in a material one. Relations may exist objectively between certain objects and not at all for others. Similarly, objects or sets of objects, which exist objectively, hold the potential for an infinite number of relationships with one another. This perspective can seem philosophically abstract, positing that any scenario is conceivable. However, we can convey this more clearly. Recall our discussion of the \emph{Cartesian product} as a set extension, where we consider two sets, \( A \) and \( B \). These sets could represent any discernible entity or indiscernible concept. Theoretically, there could be countless relationships among all elements of these sets, and the capacity of two sets to form a Cartesian product is indicative of a general type of relationship. Consequently, the elements of the power set \( P(A \times B) \) exemplify all possible cases of a certain kind of relationship.

A vivid mathematical example is the definition of Euclidean spaces, where an infinite number of subspaces can be defined, each representing a unique binary relation within the space.

We first introduce the definition of a binary relation in terms of sets. 
\begin{definition}[Binary Relation]
    A \textbf{Binary Relation} $R$ from a set $A$ to a set $B$ is a subset of the 
    Cartesian product $A \times B$. For elements $a \in A$ and $b \in B$, if the pair 
    $(a, b)$ belongs to the subset $R$, then we say $a$ is related to $b$ by the 
    relation $R$, denoted as $aRb$, whose negation is $a\not R b$.
\end{definition}

In last section, we introduced the higher abstraction of sets, which is class. Classes also have Cartesian
product,thus, we can give a general definition to all relations using class.
\begin{definition}[Relation]
    A relation is a class of ordered pairs.

    Let $R$ be a relation. We define the domain and range of $R$ to be the classes Dom $R$ and Range $R$ 
    given by
$$
\begin{array}{rcl}\text{Dom }R&=&\{x:(\exists y)((x,y)\in R)\},\\\text{Range }R&=&\{y:(\exists x)((x,y)\in R)\}.\end{array}
$$
If $R$ is a relation and $(x,y)\in R$ we say that $x$ is $R$-related to $y$ and that $y$ is an $R$-relative of $x$. 
Thus Dom $R$ is the class of all sets which $have$ $R$-relatives and Range $R$ is the class of all sets which 
$are$ $R$-relatives.
\end{definition}

Now we look into several concrete examples of relation.
\begin{example}
    Consider the set of all points on a plane. The relation defined by the equation of a circle, $x^2 + y^2 = r^2$, includes all points $(x, y)$ that satisfy this equation. This relation is not a function because, for most values of $x$, there are two possible values of $y$ that satisfy the equation, one positive and one negative (except for the points where $x = \pm r$, where there is only one value of $y$).
    
    In contrast, a function would allow each $x$ to be associated with exactly one $y$. For instance, the square function $y = x^2$ is a function because each value of $x$ corresponds to exactly one value of $y$.

    In the figure we have defined a circle and a quadratic function, clearly 
    we see that a function can never have two $y$ value for one $x$, while 
    this is possible for the circle.
    \end{example}
\begin{figure}[H]
    \centering
    \includegraphics[width = 0.75\linewidth]{function&relation.png}
    \caption{Visual Comparison of a Relation and a Function}
\end{figure}
\begin{definition}[Functional Relation]
    A relation $R$ is said to be \textbf{functional} if each element of its domain has exactly one $R$-relative; 
    a functional relation is also called a \textbf{function}. If $R$ is a functional relation then for each element 
    $a$ of its domain we denote the unique $R$-relative of $a$ by $R(a).$ 
\end{definition}

    This leads us to the next axiom of NBG theory.
    \begin{axiom}[Replacement Axiom]
        For every functional relation $R$, if the domain of $R$ is a set then the range 
        of $R$ is also a set.
    \end{axiom}

    In naive set theory and function in part 1 of the book, we gave a rough 
    definitions to mappings. With class, we can make it more concrete.
    \begin{definition}[mapping]
        A mapping is an ordered pair $((A,B),R)$ where $A$ and $B$ are sets and $R$ is a 
        functional relation between $A$ and $B$ such that Dom $R=A.$ If 
        $f= ( ( A, B) , R) $ is a mapping we say that $f$ is a mapping from 
        $A$ to $B;$ we call $A$ the domain of $f,B$ the codomain of $f$ and $R$ the 
        graph of $f$. If $f$ is a mapping with domain $A$ and codomain $B$ we often write $f:A\to B.$ 
        If $a$ is any element of the set $A$ then the set $R^{\to}(\{a\})$ consists of a single element of 
        $B$ which we denote by $f(a);$ we call it the image of $a\textbf{ under }f$ or the value of $f\textbf{ at }a.$
    \end{definition} 
    It is clear from the definition of the term “mapping” that in order to describe a mapping $f$ we must give the domain $A$ and codomain $B$ of $f$ and also, for each element $a$ of the domain we must describe the unique element $b_a$ of the codomain such that $(a,b_a)$ belongs to the graph of $f$, i.e. we must describe for each element $a$ of $A$ its image under $f$ in $B.$
    Let $f= ( ( A, B) , R) $ be a mapping from $A$ to $B$. For each subset $X$ of $A$ we denote the subset 
    $R^{\to}(X)$ of $B$ by $f^{\to}(X);$ in particular, if $a$ is any element of $A$ we have
    $f^{\to}(\{a\})=\{f(a)\}.$ For each subset $Y$ of $B$ we denote the subset $R^{\leftarrow }( Y) $ 
    of $A$ by $f^{\leftarrow }( Y) .$

    Let $ f=((A,B),R)$ be a mapping from $A$ to $B;\operatorname*{let}A_1$ be a subset of $A.$ Then the restriction of $f$ to $A_1$ is the mapping $f\mid A_1=$
    $$
    ((A_1,B),R\cap(A_1\times B)).
    $$

    Earlier, we discussed composition and inverse of function. Now that we have known that function is a kind of 
    relation, can we compose or inverse all other relations? Naturally the answer is yes.

    \begin{definition}[Composite Relation]
        If $R$ and $S$ are relations, the composition of $R$ and $S$ is the relation $S\circ R$ given by
        $$
        S\circ R=\{(x,z):(\exists y)(((x,y)\in R)\wedge((y,z)\in S))\}.
        $$
        If $R$ is a relation between $A$ and $B$ and $S$ is a relation between $B$ and $C$ then clearly 
        $S\circ R$ is a relation between $A$ and $C$, and we have $\operatorname{Dom}\left(S\circ R\right)\subseteq\operatorname{Dom}R$ and Range $(S\circ R)\subseteq\operatorname{Range}S.$
    \end{definition}

    Let $f= ( ( A, B) , R) $ and $g= ( ( B, C) , S) $ be mappings. Then clearly 
    $((A,C),S\circ R)$ is also a mapping, which we denote by $g\circ f$ and call the composition or 
    composed mapping of $f$ and $g$. For each element $a$ of $A$ we have $( g\circ f) ( a) = g( f( a) ) .$


    Similarly, we can define inverse relation.
    \begin{definition}[Inverse Relation]
        If $A$ and $B$ are classes then a relation between $A\textbf{ and }B$ is a subclass of $A\times B$, 
        i.e. a relation $R$ such that Dom $R\subseteq A$ and Range $R\subseteq B.$ A relation on a class $A$ 
        is a subclass of $A\times A.$

        If $R$ is a relation, the inverse of $R$ is the relation $R^{-1}$ given by
        $$
        R^{-1}=\{(x,y):(y,x)\in R\}.
        $$
        If $R$ is a relation between $A$ and $B$ then $R^{-1}$ is a relation between $B$ and $A;$ 
        clearly Dom $R^{-1}=\operatorname{Range}R$ and Range $R^{-1}=\operatorname{Dom}R.$
    \end{definition}

    Let $R$ be a relation, $A$ any class. Then the image of $A\textbf{ under }R$ is the class consisting of all $R$-relatives of all members of $A.$ We denote this class by $R^{\to}(A).$ Thus
    $$
    R^{\to}(A)=\{y:(\exists x)((x\in A)\land((x,y)\in R))\}.
    $$
    Again let $R$ be a relation and $\ker B$ be any class. Then the inverse image of $B$ under $R$ is the class $( R^{- 1}) ^{\to }( B) , $ which we write $R^{\leftarrow}( B) .$ Thus
    $$
    \begin{aligned}R^{\leftarrow}(B)=\{x:(\exists y)((y\in B)\land((x,y)\in R))\}.\end{aligned}
    $$
\subsection{Reflexivity, Symmetry, and Transitivity}
\subsection{Anti-symmetry and Asymmetry}
\subsection{Representation of Relation}
\subsection{Exercises}
\begin{exercise}
    Russel's Paradox is fixed by introducing more strict set axioms to the set theory. In NBG theory, a
    proper class is defined as a top-level class that cannot be subclass of any other classes or sets, and
    this is the key to avoid the paradox in naive set theory. Explain in details that how 
    this concept avoids the case in Russel's Paradox.
\end{exercise}
\begin{solution}
    With the introduction of the concept of classes, we can discuss collections 
    that are too large to be considered as sets, such as the ``class of all sets.'' 
    Therefore, the set described by Russell's Paradox, denoted by \( R \), 
    is no longer considered a legitimate set but rather a ``proper class.'' 
    This means that we refrain from discussing \( R \) as a set, thereby avoiding 
    the paradox because proper classes are not subject to the operations and 
    axioms that constrain sets. As a result, the axiomatic system does not 
    permit the formation of sets that would lead to Russell's Paradox.

\end{solution}
\begin{exercise}
    Let $A,B,C$ be classes such that $A\subseteq B,B\subseteq C$, $C\subseteq A.$ Prove that $A=B=C.$
\end{exercise}

\begin{exercise}
    Let $A,B,C$ be classes such that $A\subset B,B\subset C.$ Prove that $A\subset C.$
\end{exercise}


\section{Equivalence Relations}



\subsection{Exercises}
\section{Partial and Total Order Relations}



\subsection{Exercises}
\section{Special Types of Relations}
\subsection{Recursive Relations}
\subsection{\(n\)-ary Relations}

\subsection{Exercises}



\begin{comment}
    

\section{Searching algorithms}
    The very first thing we need to do is figure out what is searching, before we do any math.
    \begin{definition}[Searching algorithm]
        Given an array \( A[1], A[2], A[3], \ldots, A[n] \) and a target value \( T \), find an index \( j \) where \( T = A[j] \) or determine that no such index exists because \( T \) is not in the array \( A \).
    \end{definition}
    \subsection{Searching in Unordered Array List}
    We proceed to analyze the most common case, where we need to find the element $T$ from an unordered 
    array. In this case, what we need to do is just like counting numbers. We will check each element in
    the array $A$, from $A[1]$ to $A[n]$, until we find $T$. This way of reaching in pure sequential 
    order is called \textbf{Linear Search}. The pseudocode is shown below.
    \begin{algorithm}
        \caption{Linear search for a target value in an array}
        \begin{algorithmic}[1]
        \Procedure{LinearSearch}{$A, T$}
            \For{$j = 1$ to $j=n$}
                \If{$A[j] == T$}
                    \State \Return $j$
                \EndIf
            \EndFor
            \State \Return $\text{``not found''}$
        \EndProcedure
        \end{algorithmic}
        \end{algorithm} 
    This is the easiest  algorithm in this book, and it is clear that for an array of size $n$, this 
    algorithm has a time complexity of $O(n)$ in the worst cases. Now consider that, what if we are lucky
    enough to find our target $T$ in the very first element in the array? Actually, computer scientists
    have the other notation to show the best-case performance of a algorithm, which is $\omega$ notation.
    \begin{notation}[Omega Notation]
        The Omega notation describes the lower bound of an algorithm's running time. 
        It measures the best-case scenario, at least this fast. For a given function g(n), 
        we denote $\omega(g(n))$ the set of functions:
        \[
        \Omega(g(n)) = \{ f(n) : \exists c > 0, n_0 > 0 \text{ such that } 0 \leq cg(n) \leq f(n) \text{ for all } n \geq n_0 \}
        \]
    \end{notation}
    Yet there is another notation to show the overall, or average performance of an algorithm.
    \begin{notation}[Theta Notation]
        The Theta notation tightly bounds a function from above and below, meaning it defines an algorithm's running time both in the best and worst cases. For a given function $g(n)$, we denote $\Theta(g(n))$ the set of functions:
        \[
        \Theta(g(n)) = \{ f(n) : \exists c_1 > 0, c_2 > 0, n_0 > 0 \text{ such that } 0 \leq c_1g(n) \leq f(n) \leq c_2g(n) \text{ for all } n \geq n_0 \}
        \]
    \end{notation}
    Theta notation is used to evaluate the average time complexity of executing an algorithm. For instance,
    we use $\bar{p}$ to denote the average of accessed elements. We have:
    $$\bar{p} = \frac{1}{n} \sum_{i=i}^{n}i = \frac{1}{n}\times\frac{n(n+1)}{2}=\frac{n+1}{2}$$
    Which means this algorithm has $\Theta(\frac{n+1}{2})$ time complexity.
    \subsection{Searching in Ordered Array List}
    It seems that we do not have any better way to design a more efficient algorithm for finding target in 
    unordered array. Now we will discuss the case for an ordered array.
    \begin{definition}
        An ordered array refers to an array $A$ that satisfies $A[1]\leq \cdots \leq A[n]$ or $A[1]\geq 
        \cdots, \geq A[n]$.
    \end{definition}
    In this case, the target $T$ and any random element $A[n]$ in the array has the following properties
    \begin{itemize}
        \item  If \( T < A[i] \), then \( T \) cannot occur among \( A[i], A[i + 1], \ldots, A[n] \), and we need only search the list from \( A[1] \) to \( A[i - 1] \)
        \item  If \( A[i] < T \), then \( T \) cannot occur among \( A[1], A[2], \ldots, A[i] \), and we need only search the list from \( A[i + 1] \) to \( A[n] \)
    \end{itemize}
    If we adopt a policy of making the worst case as favorable as we can, we should
    probe in the middle (or as near to the middle as possible) of the list (or sublist) we're
    searching. Then, if we don't find the target there, we will have at most half the list to
    search.
    
    Suppose the partition of the original list we are searching in is from $A[p]$ to $A[q]$. We can get
    the median by $\frac{p+q}{2}$, however, when $p+q$ is odd, we cannot get a valid integer as position
    of element. As a result, we probe $A[i]$ where $i = \lfloor \frac{p+q}{2} \rfloor$ instead. 
    The resulting search algorithm where we probe in the middle of the current
    sublist is \textbf{Binary Search}.
    \begin{algorithm}[H]
        \caption{Binary Search for a target value in a sorted array}
        \begin{algorithmic}[1]
        \Procedure{BinarySearch}{$A, T$}
            \State $p \gets 1$
            \State $q \gets n$
            \While{$p \leq q$}
                \State $j \gets \left\lfloor (p + q) / 2 \right\rfloor$
                \If{$A[j] == T$}
                    \State \Return $j$
                \ElsIf{$A[j] < T$}
                    \State $p \gets j + 1$
                \Else
                    \State $q \gets j - 1$
                \EndIf
            \EndWhile
            \State \Return $\text{``not found''}$
        \EndProcedure
        \end{algorithmic}
        \end{algorithm}
    \begin{remark}
        If this code is a little confusing for you, you may check \href{https://www.cs.usfca.edu/~galles/visualization/Search.html}{here}
        for complete visualization of binary search.
    \end{remark}
    
    Walkthrough with $n = 12$ and $A = (3, 5, 8, 8, 9, 16, 29, 41, 50, 63, 64, 67)$. 

    If $T=99$, then, we can show the variables involved in a table.
    \begin{table}[h!] 
        \label{bisearch1}
        \centering
        \begin{tabular}{cccccc}
        \toprule
        \( p \) & \( j \) & \( q \) & \( A[j] \) & relation     & output         \\
        \midrule
        1       & 6       & 12      & 16         & \( A[j] < T \) & -             \\
        7       & 9       & 12      & 50         & \( A[j] < T \) & -             \\
        10      & 11      & 12      & 64         & \( A[j] < T \) & -             \\
        12      & 12      & 12      & 67         & \( A[j] < T \) & -             \\
        13      & -       & 12      & -          & -              & \( T \) is not in \( A \) \\
        \bottomrule
        \end{tabular}
        \caption{Binary Search Iterations}
        \end{table}
        Suppose the sublist we're searching is from \( A[p] \) up to \( A[q] \) of length \( k = q - p + 1\) and we probe unsuccessfully at \( A[j] \):
        $$\underbrace{A[p]\ldots A[j-1]}_{k1}\quad\text{A}[j]\quad\underbrace{A[j+1]\ldots A[q]}_{k2}$$
        
        If \( A[j] \geq T \), then we will search from \( A[p] \) up to \( A[j-1] \) of length \( k_1 = j - p \). \\
        
        If \( A[j] < T \), then we will search from \( A[j+1] \) up to \( A[q] \) of length \( k_2 = q - j \). \\
        
        We would like \( k_1 \) and \( k_2 \) to be equal, but if that's not possible (because \( q - p \) is odd), let's consistently make \( k_1 \) the smaller value.
        $\text{How should we choose }  j  \text{ so that }  k_1 \leq k_2  \text{ and }  k_2$  is as small as possible?
        To make \( k_2 = q - j \) as small as possible, we need to make \( j \) as large as possible. We want
        
        \[ k_1 + k_2 \leq k_2 + k_2, \]
        that is,
        \[ q - p \leq 2(q - j) = 2q - 2j \]
        or
        \[ 2j \leq 2q - (q - p) = q + p. \]
        
        The largest integer \( j \) such that \( j \leq (q + p)/2 \) is \( \lfloor (q + p)/2 \rfloor \), and this is the \( j \)-value used in the algorithm.
        \begin{theorem}[Binary Search Sublist Lengths] \label{sublistlen}
            On each iteration of Binary Search with \( j = \left\lfloor \frac{p + q}{2} \right\rfloor \), the lengths of the sublists will be \( k_1 = \left\lfloor \frac{k - 1}{2} \right\rfloor \) such that \( k_1 \leq k_2 = \left\lceil \frac{k - 1}{2} \right\rceil \leq \frac{k}{2} \).
            \end{theorem}
            
            \begin{proof}
            Since \( j \leq \frac{q + p}{2} < j + 1 \), subtracting \( p \) from each of these three expressions gives
            \begin{align*}
            j - p &\leq \frac{q + p}{2} - p < j - p + 1 \\
            k_1 &\leq \frac{q - p}{2} < k_1 + 1.
            \end{align*}
            Thus, \( k_1 = \left\lfloor \frac{q - p}{2} \right\rfloor = \left\lfloor \frac{k - 1}{2} \right\rfloor \).
            
            Because \( k_1 + k_2 = k - 1 \), we know that
            \begin{itemize}
            \item if \( k \) is odd, say \( k = 2r + 1 \), then \( k - 1 = 2r \) and \( k_1 = r = k_2 < \frac{k}{2} \),
            \item if \( k \) is even, say \( k = 2r \), then \( k - 1 = 2r - 1 \) and \( k_1 = r - 1 < r = k_2 = \frac{k}{2} \).
            \end{itemize}
            Thus, \( k_1 = \left\lfloor \frac{k - 1}{2} \right\rfloor \leq k_2 = \left\lceil \frac{k - 1}{2} \right\rceil \leq \frac{k}{2} \).
            \end{proof}

            \begin{theorem}[Binary Search Termination]
                Binary Search terminates after at most \(\lceil \lg(n) \rceil + 1\) probes.
                \end{theorem}
                
                \begin{proof}
                Let \( w = \lfloor \lg(n) \rfloor \). If, in some instance, Binary Search has not terminated after \( w \) (unsuccessful) probes, then the current value of \( p \) must be \( \leq \) the current value of \( q \), and the length of the current sublist, \( k \), must be \( \leq n/2^w \).
                \[ // \text{Prove this by MI.} \]
                Since \( w \leq \lg(n) < w + 1 \),
                \[ 2^w \leq n < 2 \times 2^w \]
                \[ // \text{then dividing each by } 2^w \]
                \[ 1 \leq n/2^w < 2. \]
                \[ // \text{which implies that } k = 1 \]
                
                For the next iteration, \( p = q \) and so \( j = p \).
                \[ // \text{And we probe the one remaining entry in } A. \]
                
                If \( A[j] < T \), then \( p \leftarrow j + 1 > q \), so Binary Search terminates after this probe; if \( A[j] > T \), then \( q \leftarrow j - 1 < p \), so Binary Search terminates after this probe; and if \( A[j] = T \), then Binary Search terminates after this one last probe.
                
                Therefore, Binary Search terminates after at most \(\lceil \lg(n) \rceil + 1\) probes.
                \end{proof}
    To show that the binary search algorithm is correct, we also need to show its \textbf{loop invariant}.
        \begin{definition}
            In computer science, a loop invariant is a logical assertion that is true before and after 
            each iteration of a program loop. It is a condition that is true at the beginning and 
            end of every iteration of a loop.
        \end{definition}
    Assume that $T$ is in $A$ and $T=A[i]$, $i \in [p, q]$.
    \begin{theorem}[Loop Invariance of Binary Search]
        After $k$ iterations of the loop, if $T$ = $A[i]$, then $p\leq i \leq q$.
    \end{theorem}
    \begin{proof}
        By Mathematical Induction on \(k\) where \(k \in \{0..\}\)

\textbf{Step 1.} After \(k = 0\) iterations of the loop, that is, before the loop is done once
\[ p = 1 \quad \text{and} \quad q = n, \]
and therefore,
\[ \text{if} \quad T = A[i] \quad \text{then} \quad p \leq i \leq q. \quad (\text{P}(0) \text{ is True}) \]

\textbf{Step 2.} Assume \(\exists w \in \mathbb{N}\) such that
after \(w\) iterations of the loop, if \( T = A[i] \), then \( p \leq i \leq q \). \quad (\text{This is P}(w).)

\textbf{Step 3.} Suppose there is another iteration, the \( w + 1^{st} \).
That is, \( T \) has not been found in \( w \) unsuccessful probes, and now, \( p \leq q \).
On the next iteration, we calculate a new \( j \)-value,
\[ j_{\text{new}} \leftarrow \left\lfloor \frac{(p + q)}{2} \right\rfloor \]

Since \( p \leq q \),
\[ p + p \leq p + q \leq q + q \]
and
$$p\ =\ \frac{p+p}{2} \leq \frac{p+q}{2} \leq \frac{q+q}{2} =q$$
and so \( p \leq j_{\text{new}} \leq q \).
In fact, if \( p = q \), then \( j_{\text{new}} = p \), and if \( p < q \), then \( p \leq j_{\text{new}} < q \).

In the remainder of the proof, we will let \( p^* \) and \( q^* \) denote the values of \( p \) and \( q \) at the end of the iteration. There are three cases to consider:

\textbf{Case 1.} If \( A[j_{\text{new}}] < T \), then \( T \) cannot occur at or before position \( j_{\text{new}} \) hence
\[ \text{if} \quad T = A[i] \quad \text{then} \quad p < j_{\text{new}} + 1 \leq i \leq q. \quad \text{Here} \quad p^* = j_{\text{new}} + 1 \quad \text{and} \quad q^* = q. \]

\textbf{Case 2.} If \( A[j_{\text{new}}] > T \), then \( T \) cannot occur at or after position \( j_{\text{new}} \) hence
\[ \text{if} \quad T = A[i] \quad \text{then} \quad p \leq i < j_{\text{new}} - 1 < q. \quad \text{Here} \quad p^* = p \quad \text{and} \quad q^* = j_{\text{new}} - 1. \]

\textbf{Case 3.} If \( A[j_{\text{new}}] = T \), then neither \( p \) nor \( q \) are changed hence, from Step 2
\[ \text{if} \quad T = A[i] \quad \text{then} \quad p \leq i \leq q. \quad \text{In this case,} \quad p^* = p \quad \text{and} \quad q^* = q. \]

Therefore, after this next iteration,
\[ \text{if} \quad T = A[i] \quad \text{then} \quad p^* \leq i \leq q^*. \quad \text{in every case} \]
    \end{proof}
    \begin{remark}
    When the repeat-loop in Binary Search terminates,
    \[ \text{if} \quad T = A[i], \quad \text{then} \quad p \leq i \leq q. \quad \text{(That is, this conditional statement is True.)} \]
    
    But if \( p > q \), there is no index \( i \) where \( p \leq i \leq q \).
    \[ \text{(The consequent must be False. The antecedent must be False.)} \]
    
    If \( p > q \), there is no index \( i \) where \( T = A[i] \).
    \[ \text{(If \( p > q \), then \( T \) cannot be in \( A \). Therefore, when Binary Search terminates,} \]
    
    the target has been found at position \( j \)
    or
    \[ (p > q \quad \text{and so} \quad T \text{ is not in } A.) \]
    
    Binary Search is correct and very efficient.
    \[ \text{(compared to Linear Search)} \]
    \end{remark}

    \subsection*{Branching Diagrams}
    \begin{definition}[Branching Diagram]
        A General Branching Diagram for an algorithm is a tree that shows all possible
        sequences of operations the algorithm might do.
    \end{definition}
    Branching tree is an important tool for algorithm analysis; when n is not very large, we can draw the
    branching diagram for algorithm analysis.

    Suppose that we have an array with $n=12$. In this branching tree, each cell represent a result of iterating element in the array. Certainly, we have $A[6]$ as the first element to be accessed.
    With 6 visited, there are two possible consequence of comparisons which will lead us to either
    $A[3]$ or $A[9]$, and so forth.
    \begin{figure}[H]
        \centering
        \includegraphics[width = 0.8 \textwidth]{"branchingtree.png"}
        \caption{Branching Tree of Binary Search}
    \end{figure}

    This kind of diagram is known as a \textbf{Binary Tree}: it's “rooted” at the vertex
    placed at the top of the diagram, and is binary in the sense that from any vertex,
    there are at most two edges downward in the diagram. The vertices with no
    downward edges are called \textbf{leaves}. The other vertices are called \textbf{internal vertices}.
    Every vertex, except the root, is the end of exactly one edge to it from a vertex
    above it in the diagram.

    \subsection{Binary Search: Version 2}
    Now we look into the other version of binary search.
    \begin{algorithm}[H]
        \caption{Binary Search Version 2 for a target value in a sorted array}
        \begin{algorithmic}[1]
        \Procedure{BinarySearch}{$A, T$}
            \State $p \gets 1$
            \State $q \gets n$
            \While{$p < q$}
                \State $j \gets \left\lfloor (p + q) / 2 \right\rfloor$
                \If{$A[j] < T$}
                    \State $p \gets j + 1$
                \Else
                    \State $q \gets j$
                \EndIf
            \EndWhile
            \If{$A[p] = T$}
                \State \Return $p$
            \Else
                \State \Return \textit{not found}
            \EndIf
        \EndProcedure
        \end{algorithmic}
        \end{algorithm}
    There is only one major difference between these 2 versions. For $p$ and $q$, the way they are updated in
    the second version is different, as $q$ is assigned by $j$ instead of $j-1$. It seems that this is 
    only a trivial change in the algorithm, mathematically, however, it changes the way how it works completely.
    We refer to the same example where $T=99$.
    \begin{table}[h!]
        \centering
        \caption{Execution steps of Binary Search when $T = 99$}
        \begin{tabular}{ccccccc}
        \toprule
        $p$ & $j$ & $q$ & $p < q$ & $A[j]$ & $A[j] < T$ & Output \\
        \midrule
        1 & 6 & 12 & t & 16 & t & - \\
        7 & 9 & 12 & t & 50 & t & - \\
        10 & 11 & 12 & t & 64 & t & - \\
        12 & - & 12 & f & - & - & $T$ is not in $A$ \\
        \bottomrule
        \end{tabular}
    \end{table}

    Comparing with the table \ref{bisearch1}, the number of iteration decreases by 1.

    Is this algorithm really correct? To figure it out, recall that to show an algorithm correct,
    we need to secure its loop invariance and whether it is terminable. Actually, this algorithm 
    operates on two partitions of the array only, but not 3, as in the previous version. 
    Suppose the sublist we're searching is from $A[p]$ up to $A[q]$ of length $k = q-p+1$, and we
    make the comparison “is $A[j] < T$?”:
    $$\underbrace{A[p] \ldots A[j]}_{k 1} \quad \underbrace{A[j+1] \ldots A[q]}_{k 2}$$

    In this case, $A[j]$ is not considered as an independent element, but the last element of sublist $p$ to $j$.
    If $A[j] < T$, the Procedure will try to find it in $k2$ by assign the index of the smallest item in it 
    to $p$. And if $A[j] \geq T$, it is just the other way around. Recall that in theorem \ref{sublistlen},
    when $j = \lfloor \frac{p+q}{2}\rfloor$ the length of $k1$  will be $\lfloor\frac{k-1}{2}\rfloor$, 
    while length of $k2$ is $\lceil \frac{k-1}{2}\rceil$. However, $\forall k\in \mathbb{Z}$, 
    $\lceil \frac{k-1}{2} \rceil = \lfloor \frac{k}{2}\rfloor$ (proven in previous chapter).

    Since $k_1 + k_2 = k$, we have
    \[
    \text{if } k \text{ is even, say } k = 2r, \text{ then } k_1 = r = k_2 = \frac{k}{2} \text{ and}
    \]
    \[
    \text{if } k \text{ is odd, say } k = 2r + 1, \text{ then } k_2 = r < \frac{k}{2} < r + 1 = k_1.
    \]
    In general,
    \[
    k_2 = \left\lfloor \frac{k}{2} \right\rfloor \leq \frac{k}{2} \leq k_1 = \left\lceil \frac{k}{2} \right\rceil.
    \]

    Therefore, at some iterations of binary search \#2, the length of the next sublist is more than half the length of the previous sublist.

    Let $L(w)$ denote the length of the sublist still to be searched after $w$ iterations of the while loop.
    \begin{theorem}\label{4.2.1}
        After $w$ iterations of the loop
        \[
        \left\lfloor \frac{n}{2^w} \right\rfloor \leq L(w) \leq \left\lceil \frac{n}{2^w} \right\rceil.
        \]
    \end{theorem}
    \begin{proof}
        Denote the statement as $P(w)$

        By Mathematical Induction on $w$ where $w \in \{0, \ldots\}$.

    \paragraph{Step 1.}
        After $w = 0$ iterations of the loop, before the loop is done once the length of the current sublist,
        \[
        L(0) = n = \left\lceil \frac{n}{2^0} \right\rceil = \left\lfloor \frac{n}{2^0} \right\rfloor.
        \]
        $P(0)$ is True.

        Suppose that for the first few iterations of the loop (even though $p$ and $q$ may have been changed), these bounds on the length of the current sublist are maintained.

        \paragraph{Step 2.}
        Assume there exists $m \in \mathbb{N}$ such that after $m$ iterations of the loop
        \[
        \left\lfloor \frac{n}{2^m} \right\rfloor \leq L(m) \leq \left\lceil \frac{n}{2^m} \right\rceil.
        \]
        This is $P(m)$.

        \paragraph{Step 3.}
        Suppose there is another iteration, the $m + 1$. That is, now $p < q$. We know that
        \[
        \left\lfloor \frac{L(m)}{2} \right\rfloor \leq L(m + 1) \leq \left\lceil \frac{L(m)}{2} \right\rceil.
        \]
        If we can show that
        \[
        \lceil L(m) / 2\rceil\leq\left\lceil\left\lceil n / 2^{m}\right\rceil / 2\right\rceil \text { and }\left\lceil\left\lceil n / 2^{m}\right\rceil / 2\right\rceil \leq \left\lceil n / 2^{m+1}\right\rceil
        \]
        we will have the upper bound we want: $L(m + 1) \leq \left\lceil \frac{n}{2^{m+1}} \right\rceil$.

        We deal with these (and the corresponding inequalities with the floor function) in a more general setting in the next two lemmas.

        Remember that $\left\lceil r \right\rceil$ is (defined to be) the smallest integer greater than or equal 
        to the (real) number $r$ and $\left\lfloor r \right\rfloor$ is the largest integer less than or equal 
        to $r$. Also, recall that whenever $y$ is not an integer, $\left\lfloor y \right\rfloor < y < \left\lfloor y \right\rfloor + 1$.
        
        It takes some extra step to show this. We will prove the inductive hypothesis by 2 lemmas.

        \noindent \textbf{Lemma 1}: If \( x \) and \( y \) are real numbers and \( x < y \), then \( \lceil x \rceil \leq \lceil y \rceil \) and \( \lfloor x \rfloor \leq \lfloor y \rfloor \).
        \begin{proof}
            The floor and ceiling functions are nondecreasing.
            Since \( x < y \leq \lceil y \rceil \in \mathbb{Z} \) and \( \lceil x \rceil \) is the smallest integer \( \geq x \), \( \lceil x \rceil \leq \lceil y \rceil \).
            Since \( \lfloor x \rfloor \leq x < y \) and \( \lfloor y \rfloor \) is the largest integer \( \leq y \), \( \lfloor y \rfloor \geq \lfloor x \rfloor \).
        \end{proof}

        \noindent \textbf{Lemma 2}: For any real number \(x\), \(\left\lfloor \frac{\lfloor x\rfloor}{2} \right\rfloor = \left\lfloor \frac{x}{2} \right\rfloor\) and \(\left\lceil \frac{\lceil x\rceil}{2} \right\rceil = \left\lceil \frac{x}{2} \right\rceil\).
        \begin{proof}
            If \(x\) is an integer, then \(\lfloor x\rfloor = x = \lceil x\rceil\), and therefore,
            \[
            \left\lfloor \frac{\lfloor x\rfloor}{2} \right\rfloor = \left\lfloor \frac{x}{2} \right\rfloor
            \]
            and
            \[
            \left\lceil \frac{\lceil x\rceil}{2} \right\rceil = \left\lceil \frac{x}{2} \right\rceil
            \].
            Suppose now that \(x\) is not an integer, and let \(Q\) denote \(\left\lfloor \frac{x}{2} \right\rfloor\). Then, we have
            \[
            Q < \frac{x}{2} < Q + 1 \quad // \frac{x}{2} \text{ cannot be an integer.}
            \]
            \[
            \Leftrightarrow 2Q < x < 2Q + 2.
            \]
            In fact,
            \[
            2Q \leq \lvert x\rvert < 2Q + 2,
            \]
            // because \(2Q \in \mathbb{Z}\)
            and
            \[
            2Q < \lceil x\rceil \leq 2Q + 2.
            \]
            Then,
            \[
            Q \leq \left\lfloor \frac{x}{2} \right\rfloor < \frac{x}{2} < \frac{x}{2} < \lceil x\rceil \leq Q + 1,
            \]
            and so
            \[
            \left\lfloor \frac{\lfloor x\rfloor}{2} \right\rfloor = \left\lfloor \frac{x}{2} \right\rfloor = Q
            \]
            and
            \[
            \left\lceil \frac{\lceil x\rceil}{2} \right\rceil = \left\lceil \frac{x}{2} \right\rceil = Q + 1.
            \]
        \end{proof}
    
        Returning to the proof of Theorem 4.2.1, and taking \(x = n/2^m\),  
        from Lemma 2, we have
        \[
        \left\lfloor \frac{n/2^m}{2} \right\rfloor = \left\lfloor \frac{n/2^m+1}{2} \right\rfloor \quad \text{and} \quad \left\lfloor \frac{n/2^m}{2} \right\rfloor = \left\lfloor \frac{n/2^m+1}{2} \right\rfloor.
        \]
        Since
        \[
        \left\lfloor \frac{n/2^m}{2} \right\rfloor \leq L(m) \leq \left\lceil \frac{n/2^m}{2} \right\rceil, \quad // \text{from Step 2}
        \]
        \[
        \left\lfloor \frac{n/2^m}{2} \right\rfloor \leq \frac{L(m)}{2} \leq \left\lceil \frac{n/2^m}{2} \right\rceil \quad // \text{and applying Lemma A}
        \]
        \[
        \left\lfloor \frac{n/2^m}{2} \right\rfloor \leq \frac{L(m)}{2} \quad \text{and} \quad \left\lceil \frac{L(m)}{2} \right\rceil \leq \left\lceil \frac{n/2^m}{2} \right\rceil.
        \]
        Thus,
        \[
        L(m+1) \leq \left\lfloor \frac{L(m)}{2} \right\rfloor \quad \text{and} \quad L(m+1) \geq \left\lceil \frac{L(m)}{2} \right\rceil
        \]
        that is,
        \[
        \left\lfloor \frac{n/2^m+1}{2} \right\rfloor \leq L(m+1) \leq \left\lceil \frac{n/2^m+1}{2} \right\rceil.
        \]
        The completes the proof.
    \end{proof}

    \begin{theorem}
        After at most \(\left\lceil \lg(n) \right\rceil\) comparisons of the form "Is \(A[j] < T?\)"  
        and when \(p = q\) in the last comparison, the length of the current sublist is 1.
    \end{theorem}
    \begin{proof}
        Let \(Q = \left\lceil \lg(n) \right\rceil\). Then
        \[
        Q - 1 < \lg(n) \leq Q \text{ so } 2^{Q-1} < n \leq 2^Q.
        \]
        For any integer \(k\)
        \[
        \frac{2^{Q-1}}{2^k} = \frac{2^Q}{2^{k+1}} < \frac{n}{2^k} < \frac{2^Q}{2^k} = 2^{Q-k}.
        \]
        By theorem \ref{4.2.1} and lemma 1
        \[
        L(k) \leq \left\lfloor \frac{n}{2^k} \right\rfloor \leq \frac{2^Q}{2^k} = 2^{Q-k} \quad // Q - k \in \mathbb{P} \text{ so } 2^{Q-k} \in \mathbb{P}.
        \]

        and

        \[
        L(k) \geq \left\lceil \frac{n}{2^k} \right\rceil \geq \frac{2^{Q-1}}{2^k} = 2^{Q-1-k} \quad // \text{And } 2^{Q-1-k} \in \mathbb{P}.
        \]

        In particular,

        \[
        2 = 2^1 \leq L(Q - 2) \leq 2^2
        \]

        and

        \[
        1 = 2^0 \leq L(Q - 1) \leq 2^1 = 2.
        \]

        Thus, the while loop cannot stop before \(Q - 1\) iterations are done, though it might stop after exactly \(Q - 1\) iterations are done, but if \(L(Q - 1) = 2\), it must stop after (one more iteration) exactly \(Q\) iterations.
    \end{proof}

    We can also visualize the version2 of binary search in a Branching Diagram. Below is a search
    in a sorted list with length $n=12$.
    \begin{figure}[h]
        \centering
        \includegraphics[width = 1\textwidth]{v2bintree.png}
        \caption{Binary Search V2 with $n=12$}
    \end{figure}
    This diagram is also a Binary Tree. But in it, from every internal vertex, there
    are exactly two edges downward in the diagram (never just one). This kind of
    Binary Tree is said to be a full Binary Tree. We will look into trees in detail in the next section.
    \begin{theorem}
        If \( T \) is a full binary tree with \( m \) internal vertices, then \( T \) has \( m + 1 \) leaves.
        \end{theorem}
        
        \begin{proof}
        We proceed by induction on the number of internal vertices, \( m \).
        
        \textbf{Base Case:} When \( m = 1 \), there is only one internal vertex, which is the root of the tree. By the definition of a full binary tree, the root must have two children, and since these children cannot be internal vertices (as there is only one internal vertex, the root itself), they must be leaves. Hence, the tree has \( 1 + 1 = 2 \) leaves, which satisfies our theorem.
        
        \textbf{Inductive Step:} Assume that the theorem holds for a full binary tree with \( m \) internal vertices, such that the tree has \( m + 1 \) leaves. Now consider a full binary tree with \( m + 1 \) internal vertices. By the definition of a full binary tree, adding an internal vertex means we are adding two children to an existing leaf (which is now converted into an internal vertex). This addition results in one less leaf (the converted internal vertex) and two more leaves (the new children). Therefore, the number of leaves increases by \( 2 - 1 = 1 \) compared to the number of leaves in the tree with \( m \) internal vertices.
        
        So, a tree with \( m + 1 \) internal vertices will have \( (m + 1) + 1 \) leaves. Hence, by the principle of mathematical induction, the theorem is true for all \( m \geq 1 \).
        \end{proof}
    \subsection{Exercises}
    \begin{exercise}
        Suppose \( T \) is not found by Binary Search. Where will \( T \) fit into the array? 
        Where should it be inserted?
    \begin{enumerate}
        \item Is the final \( q \)-value always one less than the final \( p \)-value?
        \item When is \( A[q] < T < A[p] \)?
        \item If \( p = 1 \) (that is, \( p \) was never changed), is \( T < A[1] \)?
        \item If \( q = n \) (that is, \( q \) was never changed), is \( A[n] < T \)?
    \end{enumerate}
    \end{exercise}
    
    \textbf{Solution}:
    \begin{enumerate}
        \item The final value of \( q \) is not necessarily always one less than the final \( p \) value. It depends on the search process and the position where \( T \) would fit if it were in the array.
        \item The condition \( A[q] < T < A[p] \) holds when \( T \) would be inserted between \( A[q] \) and \( A[p] \) in a sorted array to maintain the sorted order.
        \item If \( p = 1 \), it means \( T < A[1] \), and \( T \) should be inserted at the beginning of the array.
        \item If \( q = n \), it means \( A[n] < T \), and \( T \) should be inserted at the end of the array.
    \end{enumerate}


    \begin{exercise}
        Consider the best performance of binary search in $\Theta$ notation and worst performance in big $O$ notation. Is it possible to get the average performance in $\Theta$ notation? If yes, state the precise time complexity; if not, explain why.
    \end{exercise}
    \textbf{Solution:}

    \begin{itemize}
        \item Best time complexity: The best case scenario occurs when the target value is at the midpoint of the array, which happens with a constant time complexity, thus $\Omega(1)$. However, best case scenarios are generally not described using $\Theta$ notation as it implies a tight bound that is both the upper and lower limit, which is not applicable for the best case.
        \item Worst time complexity: Binary search has a worst-case time complexity of $O(\log n)$ when the target value is not within the initial bounds of the midpoint checks, thus requiring the maximum number of iterations to converge on the target.
        \item Average time complexity: The average time complexity of binary search is generally represented as $O(\log n)$. This is due to the fact that, on average, the number of steps required to find an element in a sorted array is logarithmic relative to the array size. Precise average case analysis of binary search is complex as it depends on the probability distribution of the target element's position. It is generally not represented in $\Theta$ notation because the lower and upper bounds for the average case cannot be tightly bounded as they can be for the best and worst cases.
    \end{itemize}
    
    \begin{exercise}
        Prove by MI that $\displaystyle \forall \ k\in \mathbb{N} ,\ $if Binary Search is applied to an array of length$\displaystyle \ n$ and has not terminated after $\displaystyle k$ (unsuccessful) probes, then the length of the current sublist must be less or equal to $\displaystyle \frac{n}{2^{k}}$
    \end{exercise}
    \begin{proof}
        We use $\displaystyle L(w)$ to denote the length of the current sublist after $\displaystyle w$ probes.
        
        \textbf{Base Case:}
        When $\displaystyle k=0$, the sorting is not yet started, so $\displaystyle L( 0) =n$, and we have $\displaystyle L( 0) =\frac{n}{2^{0}} =n$.
        
        \textbf{Inductive hypothesis:}
        Now assume that when $\displaystyle k=w$, it holds that:
        \begin{equation*}
        L( w) \leq \frac{n}{2^{w}}
        \end{equation*}
        
        \textbf{Inductive Step:}
        Thus, we need to prove that for $\displaystyle k=w+1$, $\displaystyle L( w+1) \leq 
        \frac{n}{2^{w+1}}$.
        We have proven earlier in theorem \ref{4.2.1} that after $\displaystyle w$ loops, $\displaystyle 
        \lfloor n/2^{w} \rfloor \leq L( w) \leq \lceil n/2^{w} \rceil $, and the length of the 
        sublist to be probed after $\displaystyle k+1$ unsuccessful probes is $\displaystyle 
        \frac{n}{2^{w+1}}$. While \ $\displaystyle \frac{n}{2^{w+1}} \leq \lceil n/2^{w+1} \rceil $. 
        Refer to the definition of $\displaystyle w( k)$ in the statement of problem, $\displaystyle 
        w\in \mathbb{N}$, so $\displaystyle \frac{n}{2^{w+1}} =\lceil n/2^{w+1} \rceil \ $. Hence, $\displaystyle L( w+1) \ \leq \frac{n}{2^{w+1}} \ $, this completes the proof.
    \end{proof}
    
\end{comment}

